#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —à–∞–±–ª–æ–Ω–æ–≤ —Å —Å–∞–π—Ç–∞
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ —à–∞–±–ª–æ–Ω–æ–≤
"""

import requests
import os
import re
import time
from urllib.parse import urljoin, urlparse
from pathlib import Path
import json
import hashlib

class TemplateExtractor:
    def __init__(self, base_url, output_dir='extracted_templates'):
        self.base_url = base_url.rstrip('/')
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.extracted_files = []
        self.failed_extractions = []
        
    def log(self, message, level="INFO"):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        timestamp = time.strftime("%H:%M:%S")
        print(f"[{timestamp}] {level}: {message}")
        
    def make_request(self, url, **kwargs):
        """HTTP –∑–∞–ø—Ä–æ—Å —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            time.sleep(0.5)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            response = self.session.get(url, timeout=10, **kwargs)
            return response
        except requests.exceptions.RequestException as e:
            self.log(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ {url}: {e}", "ERROR")
            return None
            
    def extract_from_directory_listing(self, directory_url):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –∏–∑ directory listing"""
        self.log(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ directory listing: {directory_url}")
        
        response = self.make_request(directory_url)
        if not response or response.status_code != 200:
            return []
            
        # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ñ–∞–π–ª—ã
        file_links = re.findall(r'href=["\']([^"\']+)["\']', response.text)
        extracted_files = []
        
        for link in file_links:
            if link.startswith('/'):
                file_url = self.base_url + link
            else:
                file_url = urljoin(directory_url, link)
                
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            if link.endswith('/') or link in ['../', './']:
                continue
                
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–π–ª
            file_content = self.extract_file(file_url)
            if file_content:
                extracted_files.append({
                    'url': file_url,
                    'filename': os.path.basename(link),
                    'size': len(file_content),
                    'method': 'directory_listing'
                })
                
        return extracted_files
        
    def extract_file(self, file_url):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞"""
        response = self.make_request(file_url)
        if response and response.status_code == 200:
            return response.content
        return None
        
    def save_file(self, content, filename, subdirectory=''):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –Ω–∞ –¥–∏—Å–∫"""
        if subdirectory:
            save_dir = self.output_dir / subdirectory
            save_dir.mkdir(exist_ok=True)
        else:
            save_dir = self.output_dir
            
        file_path = save_dir / filename
        
        try:
            with open(file_path, 'wb') as f:
                f.write(content)
            return str(file_path)
        except Exception as e:
            self.log(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {filename}: {e}", "ERROR")
            return None
            
    def extract_templates_from_source(self):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–æ–≤ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü"""
        self.log("–ü–æ–∏—Å–∫ —à–∞–±–ª–æ–Ω–æ–≤ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ...")
        
        # –ü–æ–ª—É—á–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        response = self.make_request(self.base_url)
        if not response:
            return []
            
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ CSS, JS –∏ –¥—Ä—É–≥–∏–µ —Ä–µ—Å—É—Ä—Å—ã
        css_links = re.findall(r'href=["\']([^"\']*\.css[^"\']*)["\']', response.text)
        js_links = re.findall(r'src=["\']([^"\']*\.js[^"\']*)["\']', response.text)
        image_links = re.findall(r'src=["\']([^"\']*\.(?:jpg|jpeg|png|gif|svg)[^"\']*)["\']', response.text)
        
        all_links = css_links + js_links + image_links
        extracted_files = []
        
        for link in all_links:
            if link.startswith('http'):
                file_url = link
            elif link.startswith('/'):
                file_url = self.base_url + link
            else:
                file_url = urljoin(self.base_url, link)
                
            file_content = self.extract_file(file_url)
            if file_content:
                filename = os.path.basename(urlparse(file_url).path)
                if not filename:
                    filename = f"file_{hashlib.md5(file_url.encode()).hexdigest()[:8]}"
                    
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
                if link.endswith('.css'):
                    file_type = 'css'
                elif link.endswith('.js'):
                    file_type = 'js'
                elif any(link.endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.svg']):
                    file_type = 'images'
                else:
                    file_type = 'other'
                    
                saved_path = self.save_file(file_content, filename, file_type)
                if saved_path:
                    extracted_files.append({
                        'url': file_url,
                        'filename': filename,
                        'path': saved_path,
                        'size': len(file_content),
                        'type': file_type,
                        'method': 'source_extraction'
                    })
                    
        return extracted_files
        
    def extract_from_common_paths(self):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ –æ–±—â–∏—Ö –ø—É—Ç–µ–π"""
        self.log("–ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ –ø–æ –æ–±—â–∏–º –ø—É—Ç—è–º...")
        
        common_paths = [
            '/templates/', '/themes/', '/css/', '/js/', '/images/',
            '/assets/', '/static/', '/public/', '/includes/',
            '/wp-content/themes/', '/wp-content/plugins/',
            '/admin/templates/', '/admin/css/', '/admin/js/'
        ]
        
        extracted_files = []
        
        for path in common_paths:
            url = self.base_url + path
            response = self.make_request(url)
            
            if response and response.status_code == 200:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ directory listing
                if any(indicator in response.text.lower() for indicator in [
                    'index of', 'parent directory', 'directory listing'
                ]):
                    files = self.extract_from_directory_listing(url)
                    extracted_files.extend(files)
                else:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫–∞–∫ —Ñ–∞–π–ª
                    filename = f"index_{path.replace('/', '_').strip('_')}.html"
                    saved_path = self.save_file(response.content, filename, 'directories')
                    if saved_path:
                        extracted_files.append({
                            'url': url,
                            'filename': filename,
                            'path': saved_path,
                            'size': len(response.content),
                            'method': 'common_path'
                        })
                        
        return extracted_files
        
    def extract_from_sitemap(self):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ sitemap.xml"""
        self.log("–ü–æ–∏—Å–∫ sitemap.xml...")
        
        sitemap_urls = [
            '/sitemap.xml',
            '/sitemap_index.xml',
            '/sitemaps.xml',
            '/sitemap/sitemap.xml'
        ]
        
        extracted_files = []
        
        for sitemap_url in sitemap_urls:
            url = self.base_url + sitemap_url
            response = self.make_request(url)
            
            if response and response.status_code == 200:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏–∑ sitemap
                urls = re.findall(r'<loc>(.*?)</loc>', response.text)
                
                for url in urls:
                    if url.startswith(self.base_url):
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —Ñ–∞–π–ª–æ–º —à–∞–±–ª–æ–Ω–∞
                        if any(url.endswith(ext) for ext in ['.css', '.js', '.html', '.php', '.tpl']):
                            file_content = self.extract_file(url)
                            if file_content:
                                filename = os.path.basename(urlparse(url).path)
                                saved_path = self.save_file(file_content, filename, 'sitemap')
                                if saved_path:
                                    extracted_files.append({
                                        'url': url,
                                        'filename': filename,
                                        'path': saved_path,
                                        'size': len(file_content),
                                        'method': 'sitemap'
                                    })
                                    
        return extracted_files
        
    def run_extraction(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"""
        self.log(f"–ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–æ–≤ —Å {self.base_url}")
        
        all_extracted = []
        
        # –†–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        methods = [
            self.extract_templates_from_source,
            self.extract_from_common_paths,
            self.extract_from_sitemap
        ]
        
        for method in methods:
            try:
                files = method()
                all_extracted.extend(files)
                self.log(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤ –º–µ—Ç–æ–¥–æ–º {method.__name__}")
            except Exception as e:
                self.log(f"–û—à–∏–±–∫–∞ –≤ –º–µ—Ç–æ–¥–µ {method.__name__}: {e}", "ERROR")
                
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'base_url': self.base_url,
            'extraction_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_files': len(all_extracted),
            'files': all_extracted
        }
        
        metadata_path = self.output_dir / 'extraction_metadata.json'
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
            
        self.log(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(all_extracted)}")
        self.log(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.output_dir}")
        
        return all_extracted
        
    def print_summary(self, extracted_files):
        """–í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"""
        print("\n" + "="*60)
        print("–°–í–û–î–ö–ê –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –®–ê–ë–õ–û–ù–û–í")
        print("="*60)
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º —Ñ–∞–π–ª–æ–≤
        file_types = {}
        for file_info in extracted_files:
            file_type = file_info.get('type', 'unknown')
            if file_type not in file_types:
                file_types[file_type] = []
            file_types[file_type].append(file_info)
            
        for file_type, files in file_types.items():
            print(f"\n{file_type.upper()}: {len(files)} —Ñ–∞–π–ª–æ–≤")
            total_size = sum(f['size'] for f in files)
            print(f"  –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size:,} –±–∞–π—Ç")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 —Ñ–∞–π–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
            for file_info in files[:5]:
                print(f"  - {file_info['filename']} ({file_info['size']:,} –±–∞–π—Ç)")
                
            if len(files) > 5:
                print(f"  ... –∏ –µ—â–µ {len(files) - 5} —Ñ–∞–π–ª–æ–≤")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–æ–≤ —Å —Å–∞–π—Ç–∞')
    parser.add_argument('--url', default='https://100200.ru', help='URL –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è')
    parser.add_argument('--output', default='extracted_templates', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è')
    
    args = parser.parse_args()
    
    print("üìÅ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–æ–≤ —Å —Å–∞–π—Ç–∞")
    print(f"üéØ –¶–µ–ª—å: {args.url}")
    print(f"üìÇ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤: {args.output}")
    print("-" * 60)
    
    extractor = TemplateExtractor(args.url, args.output)
    
    try:
        extracted_files = extractor.run_extraction()
        extractor.print_summary(extracted_files)
        
    except KeyboardInterrupt:
        print("\n‚ùå –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()
